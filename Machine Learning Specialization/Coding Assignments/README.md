## [C2W3 Advice for applying machine learning](https://github.com/tinghe14/-Coursera-/blob/main/Machine%20Learning%20Specialization/Coding%20Assignments/C2_W3_Assignment.ipynb)
- data split, tuning optimal degree and regularization
- under overfitting, geting more data can help

## [C2W3 Decision Tree](https://github.com/tinghe14/-Coursera-/blob/main/Machine%20Learning%20Specialization/Coding%20Assignments/C2_W4_Decision_Tree_with_Markdown.ipynb)
- implement decision tree from scratch: caculate entropy-> information gain -> split the data given feature -> return the best feature to split -> build tree
- entropy: where p_1 is the fraction of examples that have value =1 $$H(p_1) = -p_1 \text{log}_2(p_1) - (1- p_1) \text{log}_2(1- p_1)$$ 

## [C3W1 K-means](https://github.com/tinghe14/-Coursera-/blob/main/Machine%20Learning%20Specialization/Coding%20Assignments/C3_W1_KMeans_Assignment.ipynb) & [C3W1 Anomaly Detection](https://github.com/tinghe14/-Coursera-/blob/main/Machine%20Learning%20Specialization/Coding%20Assignments/C3_W1_Anomaly_Detection.ipynb)
- implement K-means from scratch
- image compression with K-means
- implment gaussian anomaly detection
